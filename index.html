<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>MVP-LAM | Cross-Viewpoint Reconstruction</title>
    <meta
      name="description"
      content="MVP-LAM learns action-centric discrete latent actions from time-synchronized multi-view videos via cross-viewpoint reconstruction."
    />

    <meta property="og:title" content="MVP-LAM" />
    <meta
      property="og:description"
      content="Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction"
    />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://latentactionpretraining.github.io/" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />

    <style>
      :root {
        --bg: #0b0c10;
        --panel: #111318;
        --text: #e9eef5;
        --muted: #aab4c3;
        --line: rgba(255, 255, 255, 0.10);
        --accent: #7aa2ff;
        --accent2: #a6ffcb;
        --shadow: 0 10px 30px rgba(0, 0, 0, 0.40);
      }

      * { box-sizing: border-box; }
      html, body { height: 100%; }
      body {
        margin: 0;
        font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
        background:
          radial-gradient(1200px 700px at 20% 0%, rgba(122, 162, 255, 0.22), transparent 60%),
          radial-gradient(900px 600px at 85% 15%, rgba(166, 255, 203, 0.12), transparent 55%),
          var(--bg);
        color: var(--text);
        line-height: 1.55;
      }

      a { color: inherit; text-decoration: none; }
      a:hover { text-decoration: underline; }

      .wrap {
        width: min(1100px, calc(100% - 40px));
        margin: 0 auto;
        padding: 26px 0 64px;
      }

      header {
        display: grid;
        gap: 16px;
        padding: 22px 0 10px;
      }

      .topbar {
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 12px;
        flex-wrap: wrap;
      }

      .badge {
        display: inline-flex;
        align-items: center;
        gap: 10px;
        padding: 8px 12px;
        border: 1px solid var(--line);
        border-radius: 999px;
        background: rgba(255, 255, 255, 0.03);
        width: fit-content;
      }
      .dot {
        width: 9px;
        height: 9px;
        border-radius: 999px;
        background: var(--accent);
        box-shadow: 0 0 0 6px rgba(122, 162, 255, 0.15);
      }
      .badge span {
        color: var(--muted);
        font-size: 13px;
        font-weight: 500;
      }

      .nav {
        display: flex;
        gap: 10px;
        flex-wrap: wrap;
      }
      .nav a {
        font-size: 13px;
        color: var(--muted);
        border: 1px solid var(--line);
        background: rgba(255, 255, 255, 0.02);
        padding: 7px 10px;
        border-radius: 999px;
      }
      .nav a:hover { color: var(--text); text-decoration: none; }

      h1 {
        margin: 0;
        font-size: clamp(28px, 4vw, 44px);
        letter-spacing: -0.02em;
        line-height: 1.15;
      }
      .subtitle {
        margin: 0;
        color: var(--muted);
        font-size: clamp(15px, 2vw, 18px);
        max-width: 95ch;
      }

      .meta {
        display: grid;
        gap: 8px;
        margin-top: 2px;
      }
      .authors {
        color: var(--text);
        font-weight: 600;
        flex-wrap: wrap;
      }
      .affils {
        color: var(--muted);
        font-size: 14px;
      }

      .cta {
        display: flex;
        flex-wrap: wrap;
        gap: 10px;
        margin-top: 10px;
      }
      .btn {
        display: inline-flex;
        align-items: center;
        gap: 10px;
        padding: 10px 14px;
        border-radius: 12px;
        border: 1px solid var(--line);
        background: rgba(255, 255, 255, 0.03);
        box-shadow: var(--shadow);
        font-weight: 600;
        font-size: 14px;
        cursor: pointer;
        user-select: none;
      }
      .btn:hover { background: rgba(255, 255, 255, 0.06); text-decoration: none; }
      .btn.primary {
        border-color: rgba(122, 162, 255, 0.50);
        background: linear-gradient(180deg, rgba(122, 162, 255, 0.20), rgba(122, 162, 255, 0.06));
      }
      .btn.disabled {
        opacity: 0.55;
        cursor: not-allowed;
        box-shadow: none;
      }
      .btn .pill {
        padding: 3px 8px;
        border-radius: 999px;
        border: 1px solid var(--line);
        color: var(--muted);
        font-weight: 600;
        font-size: 12px;
      }

      .layout {
        display: grid;
        grid-template-columns: 1.35fr 0.65fr;
        gap: 18px;
        margin-top: 16px;
      }
      @media (max-width: 980px) {
        .layout { grid-template-columns: 1fr; }
      }

      .card {
        background: rgba(255, 255, 255, 0.03);
        border: 1px solid var(--line);
        border-radius: 16px;
        box-shadow: var(--shadow);
        padding: 18px;
      }

      .card h2 {
        margin: 0 0 10px;
        font-size: 16px;
        letter-spacing: -0.01em;
      }

      .card h3 {
        margin: 18px 0 8px;
        font-size: 14px;
        letter-spacing: -0.01em;
        color: var(--text);
      }

      .card p {
        margin: 0;
        color: var(--muted);
        font-size: 14px;
      }

      .section { margin-top: 18px; }

      .figure {
        display: grid;
        gap: 10px;
        margin-top: 12px;
      }
      .figure img {
        width: 100%;
        height: auto;
        border-radius: 14px;
        border: 1px solid var(--line);
        background: rgba(255, 255, 255, 0.02);
      }
      .figcap {
        color: var(--muted);
        font-size: 13px;
      }

      .tablewrap {
        margin-top: 10px;
        overflow: auto;
        border: 1px solid var(--line);
        border-radius: 12px;
        background: rgba(255, 255, 255, 0.02);
      }
      table {
        width: 100%;
        border-collapse: collapse;
        font-size: 13px;
        min-width: 780px;
      }
      th, td {
        padding: 10px 10px;
        border-bottom: 1px solid var(--line);
        text-align: left;
        vertical-align: middle;
        white-space: nowrap;
      }
      th {
        color: var(--text);
        font-weight: 700;
        background: rgba(255, 255, 255, 0.03);
      }
      td { color: var(--muted); }
      tr:last-child td { border-bottom: none; }

      .strong { color: var(--text); font-weight: 700; }
      .u { text-decoration: underline; text-decoration-thickness: 1px; text-underline-offset: 3px; }

      .bibtex {
        margin-top: 10px;
        background: #0a0b0f;
        border: 1px solid var(--line);
        border-radius: 12px;
        padding: 12px;
        overflow: auto;
        font-size: 12px;
        color: #dfe7ff;
      }

      .note {
        margin-top: 10px;
        padding: 10px 12px;
        border: 1px solid var(--line);
        border-radius: 12px;
        background: rgba(255, 255, 255, 0.02);
        color: var(--muted);
        font-size: 13px;
      }

      footer {
        margin-top: 22px;
        color: var(--muted);
        font-size: 12px;
        display: flex;
        justify-content: space-between;
        flex-wrap: wrap;
        gap: 10px;
        border-top: 1px solid var(--line);
        padding-top: 16px;
      }

      .smalllink { color: var(--muted); }
      .smalllink:hover { color: var(--text); }

      .sr-only {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0,0,0,0);
        white-space: nowrap;
        border: 0;
      }

      code {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        font-size: 12px;
        color: #dfe7ff;
      }
    </style>

    <script>
      window.MathJax = {
        tex: { inlineMath: [["$", "$"], ["\\(", "\\)"]] },
        options: { skipHtmlTags: ["script","noscript","style","textarea","pre","code"] }
      };
    </script>
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>

  <body>
    <div class="wrap">
      <header>
        <div class="topbar">
          <div class="badge" aria-label="project status">
            <div class="dot" aria-hidden="true"></div>
            <span>ArXiv Preprint</span>
          </div>

          <nav class="nav" aria-label="page">
            <a href="#abstract">Abstract</a>
            <a href="#introduction">Introduction</a>
            <a href="#method">Method</a>
            <a href="#experiments">Experiments</a>
            <a href="#tables">Tables</a>
            <a href="#figures">Figures</a>
            <a href="#bibtex">BibTeX</a>
          </nav>
        </div>

        <h1>MVP-LAM</h1>
        <p class="subtitle">
          Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction
        </p>

        <div class="meta">
          <div class="authors">
            Jung Min Lee · Dohyeok Lee · Seokhun Ju · Taehyun Cho · Jin Woo Koo · Li Zhao · Sangwoo Hong · Jungwoo Lee
          </div>
          <div class="affils">
            Seoul National University · Konkuk University · Microsoft Research Asia · HodooAI Labs
          </div>
        </div>

        <div class="cta" role="group" aria-label="links">
          <a class="btn primary disabled" href="#" aria-disabled="true" tabindex="-1">
            <span>arXiv</span>
            <span class="pill">Coming soon</span>
          </a>
          <a class="btn disabled" href="#" aria-disabled="true" tabindex="-1">
            <span>Code</span>
            <span class="pill">Coming soon</span>
          </a>
          <button class="btn" id="copy-bib" type="button">
            <span>Copy BibTeX</span>
            <span class="pill">1 click</span>
          </button>
        </div>
      </header>

      <div class="layout">
        <main class="card">
          <h2 id="abstract">Abstract</h2>
          <p>
            Learning <em>latent actions</em> from diverse human videos enables scaling robot learning beyond embodiment-specific robot datasets, and these latent actions have recently been used as pseudo-action labels for vision-language-action (VLA) model pretraining.
            To make VLA pretraining effective, latent actions should contain information about the underlying agent's actions despite the absence of ground-truth labels.
            We propose <strong>M</strong>ulti-<strong>V</strong>iew<strong>P</strong>oint <strong>L</strong>atent <strong>A</strong>ction <strong>M</strong>odel (<strong>MVP-LAM</strong>), which learns discrete latent actions that are highly informative about ground-truth actions from time-synchronized multi-view videos.
            MVP-LAM trains latent actions with a <em>cross-viewpoint reconstruction</em> objective, so that a latent action inferred from one view must explain the future in another view, reducing reliance on viewpoint-specific cues.
            On Bridge V2, MVP-LAM produces more action-centric latent actions, achieving higher mutual information with ground-truth actions and improved action prediction, including under out-of-distribution evaluation.
            Finally, pretraining VLAs with MVP-LAM latent actions improves downstream manipulation performance on the SIMPLER and LIBERO-Long benchmarks.
          </p>

          <div class="section">
            <h2 id="introduction">Introduction</h2>
            <p>
              Collecting real-world robot demonstrations remains a central bottleneck in training generalist manipulation policies~\cite{lfvsurvey}.
              Unlike foundation models in other domains, robot learning is constrained by the cost of acquiring action-labeled trajectories, which typically requires human teleoperation.
              This makes large-scale data collection slow and expensive, and the resulting datasets often depend on a specific embodiment and sensor setup.
              To alleviate this limitation, learning from video (LfV) has emerged as a promising alternative that exploits abundant human manipulation videos to acquire transferable priors over manipulation-relevant dynamics.
              A fundamental challenge, however, is that such videos do not provide low-level robot action labels, preventing standard supervised imitation learning.
            </p>

            <div class="section">
              <p>
                To address missing actions, recent methods learn <em>latent actions</em>, compact representations of video frame transitions, and use them as pseudo-action labels~\cite{lapa,moto,univla,uniskill,villa}.
                A latent action model (LAM) learns such representations from unlabeled videos by encoding frame-to-frame transitions and optimizing a reconstruction loss to predict the next observation from the current observation and the latent action.
                These pseudo-labels have been used to pretrain vision-language-action (VLA) models and to define reusable skills for downstream control.
              </p>
            </div>

            <div class="section">
              <p>
                For effective VLA pretraining, the key requirement is that latent actions remain strongly informative about the underlying actions even when ground-truth actions are unavailable.
                Motivated by this, we define an <em>action-centric latent action</em> as one that preserves high mutual information (MI) with the true action.
              </p>
            </div>

            <div class="section">
              <p>
                A key obstacle for action-centric latent actions is <em>exogenous noise</em>, where visual transitions can be spuriously influenced by factors other than the agent's actions yet still correlate with frame-to-frame changes, e.g., people moving in the background~\cite{lam_distractor_theory, laom}.
                Among these factors, we focus on viewpoint variation.
                Viewpoint changes introduce camera movements and perspective shifts, entangling visual transitions with the agent's action.
                As a result, latent actions learned from single-view reconstruction can overfit to viewpoint-dependent cues and become less predictive of the actions.
              </p>
            </div>

            <div class="section">
              <p>
                We propose <strong>M</strong>ulti-<strong>V</strong>iew<strong>P</strong>oint <strong>L</strong>atent <strong>A</strong>ction <strong>M</strong>odel (MVP-LAM), which learns discrete latent actions that are highly informative about ground-truth actions.
                MVP-LAM is trained on time-synchronized multi-view videos with a <em>cross-viewpoint reconstruction</em> objective, where a latent action inferred from one view is used to predict the future observation in another view.
                This discourages latent actions from encoding the viewpoint-specific information and achieves more action-centric latent actions.
              </p>
            </div>

            <div class="section">
              <p>
                Empirically, MVP-LAM learns more action-centric latent actions than LAMs trained on single-view data with pixel-reconstruction objectives.
                On Bridge V2~\cite{bridgev2}, MVP-LAM achieves higher mutual information between latent actions and ground-truth actions and enables better action prediction accuracy with a simple single linear layer, including under out-of-distribution (OOD) datasets.
                Finally, VLAs pretrained with MVP-LAM latent actions outperform baselines on the SIMPLER \cite{simpler} and LIBERO-Long \cite{libero} benchmarks.
              </p>
            </div>
          </div>

          <div class="section">
            <h2 id="method">Method</h2>

            <h3>Problem Formulation</h3>
            <p>
              We denote a video by a sequence of images $\left\{ I_t\right\}_{t=1}^T$.
              For each timestep $t$, we assume that the image $I_t$ is generated under a camera pose $v_t$.
              For each image $I_t$, we extract a visual observation in a feature space as $o_t = f(I_t)$, where $f(\cdot)$ is a visual encoder such as DINOv2~\cite{dinov2} or MAE~\cite{mae}.
              Since video datasets may have different frame rates, we define a fixed temporal stride $H$ and set $o_{t+1} = f(I_{t+H})$.
            </p>

            <h3>Latent action model</h3>
            <p>
              LAM is generally implemented as a vector-quantized variational autoencoder (VQ-VAE)~\cite{vqvae}, with VLA training in mind.
              LAM learns a latent action $z_t$ that summarizes the transition from $o_t$ to $o_{t+1}$.
              Concretely, an encoder produces a continuous latent $e_t = E_\theta(o_t, o_{t+1})$, which is vector-quantized into a codebook entry, i.e.,
              $z_t = \mathrm{Quantize}(e_t)$.
              A decoder then predicts the next observation feature as $\hat{o}_{t+1} = D_\theta(o_t, z_t)$.
              In standard LAM training, the decoder does not take the viewpoint $v_t$ as input.
              The training objective is
              $$\mathcal{L}_\theta(o_t, o_{t+1})
              = \lVert o_{t+1} - \hat{o}_{t+1} \rVert_2^2
              + \mathcal{L}_\text{quant}
              + \mathcal{L}_\text{commit}.$$
            </p>

            <div class="note">
              This page renders the same math via MathJax, and keeps the original LaTeX symbols as written
            </div>

            <h3>Action-centric Latent Action</h3>
            <p>
              When latent actions are used as pseudo-action labels for behavior cloning policies, it is desirable that the learned latent action $Z_t$ preserves as much information as possible about the underlying action $A_t$.
              We denote the state by $S_t$, and assume an expert policy induces actions $A_t \sim \pi^\star(\cdot \mid S_t)$ for a given task.
              In the pretraining stage, we typically do not observe $S_t$ or $A_t$.
              Instead, we only observe images (or their features) $O_t = f(I_t)$.
              LAM produces latent actions from consecutive observations, i.e., $Z_t = E_\theta(O_t, O_{t+1})$ (with vector quantization when using VQ-VAE).
            </p>

            <div class="section">
              <p>
                Motivated by \citet{latentaction_learn}, we define a latent action $Z_t$ <em>action-centric</em> if it is highly informative about the underlying action $A_t$.
                We quantify this by mutual information and consider the objective
                $$\max_{Z_t} \ \mathcal{I}(Z_t; A_t).$$
                In this context, viewpoint variation acts as noise.
                Changes in camera pose $V_t$ can induce frame-to-frame differences in $O_t$ that are predictive of $Z_t$ but are not caused by the action $A_t$.
                When $Z_t$ is learned under a limited-capacity bottleneck such as vector quantization, allocating representational capacity to viewpoint-dependent factors can come at the expense of action-relevant dynamics and reduce $\mathcal{I}(Z_t;A_t)$.
                Under simplifying assumptions detailed in Appendix~\ref{sec:mi_analysis}, one can derive a lower bound
                $$\mathcal{I}(Z_t;A_t) \ge \mathcal{H}(Z_t) - \mathcal{I}(Z_t;V_t,V_{t+1}\mid S_t,S_{t+1}) - C$$
                where $C$ is a constant independent of the latent action $Z_t$.
                This bound suggests that when $\mathcal{H}(Z_t)$ is constrained, decreasing $\mathcal{I}(Z_t;V_t,V_{t+1}\mid S_t,S_{t+1})$ can improve action-centricity.
                This motivates using time-synchronized multi-view videos together with a cross-viewpoint reconstruction objective to discourage viewpoint-dependent factors in $Z_t$.
              </p>
            </div>

            <h3>Multi-Viewpoint Latent Action Learning</h3>
            <p>
              Building on this motivation, we introduce MVP-LAM, which leverages time-synchronized multi-view videos and cross-viewpoint reconstruction to learn action-centric latent actions.
              Although single-view capture is more convenient to collect than multi-view, it remains practical at scale for human videos \cite{tcn}, and various multi-view human datasets are readily available \cite{h2o, havid, assembly101, egoexo4d}.
              For clarity, we describe the two-view case but note that the objective extends to more views.
            </p>

            <div class="section">
              <p>
                Given time-synchronized image pairs $\{(I_t^{v_1}, I_t^{v_2})\}_{t=1}^{T}$, we first extract visual features
                $o_t^{v} = f(I_t^{v})$ using DINOv2, producing object-centric observation features.
                For each viewpoint $v \in \{v_1, v_2\}$, the encoder $E_\theta$ predicts a latent action from consecutive observations
                $$e_t^{v} = E_\theta(o_t^{v}, o_{t+1}^{v}), \quad z_t^{v} = \mathrm{Quantize}(e_t^{v}).$$
                As in standard LAMs, the decoder $D_\theta$ is trained to predict the next observation from the current observation and a latent action.
                To reduce the effect of viewpoint variation during LAM training, MVP-LAM optimizes two complementary reconstruction terms
                (i) <strong>self-viewpoint reconstruction</strong>
                (ii) <strong>cross-viewpoint reconstruction</strong>
              </p>
            </div>

            <div class="section">
              <p>
                Formally, for two synchronized views $\{v_1,v_2\}$, these terms are defined as
                $$\mathcal{L}_{\text{self}}
                = \sum_{v \in \{v_1, v_2\}} \left\lVert o_{t+1}^{v} - D_\theta(o_t^{v}, z_t^{v}) \right\rVert_2^2,$$
                $$\mathcal{L}_{\text{cross}}
                = \sum_{\substack{v,\tilde v \in \{v_1, v_2\}\\ v \neq \tilde v}}
                \left\lVert o_{t+1}^{v} - D_\theta(o_t^{v}, z_t^{\tilde v}) \right\rVert_2^2.$$
                The full objective of MVP-LAM is
                $$\mathcal{L}_{\text{MVP-LAM}}
                = \mathcal{L}_{\text{self}} + \mathcal{L}_{\text{cross}}
                + \mathcal{L}_{\text{quant}} + \mathcal{L}_\text{commit}.$$
              </p>
            </div>

            <div class="section">
              <p>
                We briefly relate cross-viewpoint reconstruction to conditional mutual information in Equation \ref{eqn:mi_lower_bound}.
                Reducing $\mathcal{L}_{\text{self}}$ and $\mathcal{L}_{\text{cross}}$ enforces
                $D_\theta(o_t^{v}, z_t^{v}) \approx D_\theta(o_t^{v}, z_t^{\tilde v})$ for $v \neq \tilde v$.
                Since the decoder is not conditioned on the viewpoint of the latent action, any viewpoint-specific factors encoded in $z_t^{v}$ would increase the cross-viewpoint reconstruction loss.
                Minimizing $\mathcal{L}_{\text{cross}}$ therefore discourages $z_t^{v}$ from encoding information that is specific to $(V_t,V_{t+1})$ beyond what is determined by $(S_t,S_{t+1})$.
                Equivalently, it reduces viewpoint dependence in $Z_t$ and thereby decreases the conditional mutual information $\mathcal{I}(Z_t;V_t,V_{t+1}\mid S_t,S_{t+1})$.
              </p>
            </div>
          </div>

          <div class="section">
            <h2 id="experiments">Experiments</h2>
            <p>
              We evaluate whether MVP-LAM learns action-centric discrete latent actions and whether these latent actions serve as effective pseudo-labels for VLA pretraining.
              Specifically, we address three questions
              <strong>RQ1.</strong> Are MVP-LAM latent actions more action-centric
              <strong>RQ2.</strong> Do they improve downstream manipulation performance after VLA finetuning
              <strong>RQ3.</strong> Do they preserve transition-relevant information under viewpoint perturbations
            </p>

            <h3>Baselines</h3>
            <p>
              We compare MVP-LAM against the following three representative LAMs.
              We provide details of the baselines in Appendix \ref{sec:detail_of_lam_baselines}.
            </p>
            <div class="note">
              UniVLA~\cite{univla} learns discrete task-relevant latent action tokens with a VQ bottleneck by encoding consecutive DINOv2 features.
              LAPA~\cite{lapa} discretizes observation transitions using a VQ-VAE latent action quantizer.
              Moto~\cite{moto} learns a latent motion tokenizer that maps videos to sequences of discrete motion tokens with a large VQ codebook.
            </div>

            <h3>Implementation details</h3>
            <p>
              MVP-LAM follows the UniVLA LAM architecture.
              For the training dataset, we use time-synchronized multi-view robot trajectories from Open X-Embodiment (OXE)~\cite{oxe}, using the OpenVLA training mixture~\cite{openvla}, and additionally include multi-view human manipulation videos from EgoExo4D~\cite{egoexo4d}.
              Overall, the training set contains 312k trajectories and we train for 160k steps.
              The full data mixture and training details of MVP-LAM are provided in Appendix \ref{sec:lam_detail}.
            </p>

            <h3>Are MVP-LAM latent actions more action-centric</h3>
            <p>
              We evaluate how action-centric a latent action is by measuring (i) mutual information between latent actions and ground-truth actions, and (ii) how well actions can be predicted from latent actions with a simple linear layer.
            </p>

            <div class="section">
              <p>
                Different LAMs operate at different temporal strides $H$.
                To make $A_{t:t+H}$ comparable, we convert per-step actions into a <em>net relative action</em> over each model’s horizon by undoing the dataset-specific normalization, aggregating over the horizon, and re-normalizing with original statistics.
                We provide the details of this process in Appendix \ref{sec:action_centric_detail}.
              </p>
            </div>

            <div class="section">
              <p>
                On Bridge V2, we estimate $\mathcal{I}(Z;A)$ using three estimators.
                the nonparametric Kraskov--St\"ogbauer--Grassberger (KSG) estimator, and two variational estimators (Barber--Agakov (BA)~\cite{ba} and a MINE style bound~\cite{mine}).
                We use $k{=}5$ for KSG.
                Since KSG is unstable in high dimensions, we apply a random projection to the latent actions so that the overall latent action dimension, including the code length, becomes $d{=}256$ before KSG.
                We provide details of MI evaluation in Appendix \ref{sec:action_centric_detail}.
              </p>
            </div>

            <div class="section">
              <p>
                To evaluate the inclusion of ground-truth actions in the latent actions, we use linear probing as \citet{laom}.
                Linear probing evaluates how much information is readily accessible in a representation by fitting a simple readout model on top of frozen features \cite{linear_probe}.
                Here, we freeze the LAM and train a lightweight probe to predict ground-truth actions from latent actions.
                We use a linear layer $\hat{a}_t = Wz_t + b$, where $W$ is the weight matrix and $b$ is the bias term.
                We report normalized mean squared error (NMSE), defined as $\mathbb{E}\|a_t-\hat{a}_t\|_2^2 / \mathrm{Var}(a)$.
                To standardize representation dimensionality across methods, we apply PCA to latent actions and keep $d{=}128$ components, including the code length.
              </p>
            </div>

            <div class="section">
              <p>
                As shown in Figure \ref{fig:mi}, MVP-LAM achieves the highest estimated $\mathcal{I}(Z;A)$ across all estimators, suggesting that its latent actions preserve more information about the actions than the baselines.
                Consistent with MI estimation, Figure \ref{fig:nmse_probe} shows that MVP-LAM achieves lower NMSE on Bridge V2 and on OOD LIBERO suites (Spatial, Object, and Long), with a small drop on LIBERO-Goal relative to UniVLA.
                Overall, MI estimation and probing consistently indicate that MVP-LAM learns more action-centric latent actions.
                We note that UniVLA may struggle to achieve action-centricity because its training objective is primarily driven by task information from language descriptions, which are typically trajectory-level, and this provides weaker supervision for encoding step-level action signals in $z_t$.
                The details of linear probing and extended analysis including LAPA and Moto are listed in Appendix \ref{sec:action_centric_detail}.
              </p>
            </div>

            <h3>Is MVP-LAM Effective for Manipulation</h3>
            <p>
              To examine whether VLA pretrained with MVP-LAM benefits from its action-centricity, we evaluate downstream manipulation on SIMPLER and LIBERO-Long with a single image and natural language description.
              Figure~\ref{fig:pretrain_finetune_detail} shows example demonstrations from both benchmarks.
            </p>

            <div class="section">
              <p>
                SIMPLER has been shown to correlate with real-world performance even though it is simulation-based.
                We evaluate four SIMPLER tasks using a 7-DoF WidowX arm to assess generalization across diverse manipulation goals.
                StackG2Y (stack the green cube on the yellow block), Carrot2Plate (place the carrot on the plate), Spoon2Towel (place the spoon on the towel), and Eggplant2Bask (place the eggplant in the basket).
                Since SIMPLER does not provide an official finetuning dataset, we use 100 diverse trajectories collected by \citet{lapa} (25 per task) and report both grasp rate and success rate.
              </p>
            </div>

            <div class="section">
              <p>
                LIBERO-Long evaluates the long-horizon manipulation performance which is the most challenging subset of LIBERO suites.
                The evaluation consists of a suite of 10 long-horizon tasks with natural language goal descriptions.
                For each task, we evaluate 10 runs with 5 random seeds, and the results are reported as the average of success rate over all 10 tasks.
              </p>
            </div>

            <div class="section">
              <p>
                We compare VLA pretrained by MVP-LAM latent actions against the following baselines.
                We provide the implementation details of the baselines in Appendix \ref{sec:detail_of_vla_baselines}.
                UniVLA~\cite{univla} pretrained on Bridge V2 is our primary baseline since the only difference is the LAM used in VLA pretraining.
                In addition, we include LAPA~\cite{lapa} which is a representative latent action based VLA.
                OpenVLA~\cite{openvla} is a VLA model that leverages a large-scale pretraining dataset, including OXE.
                Octo~\cite{octo} is transformer-based policy baselines trained on diverse robotic datasets with a unified action representation.
                Finally, we include $\pi_{0}$ ~\cite{pi0} which is state-of-the-art VLA model.
              </p>
            </div>

            <div class="section">
              <p>
                Figure \ref{fig:pretrain_finetune_detail} shows the details of VLM pretraining and VLA finetuning.
                We pretrain a VLM to predict MVP-LAM latent actions using a CE objective.
                We start from a Prismatic-7B VLM checkpoint~\cite{prismatic} and pretrain on Bridge V2.
                We then convert the pretrained VLM into a VLA by finetuning with LoRA to predict the ground-truth robot action $a_t$.
                To predict continuous robot action from discrete VLM outputs, we follow the action prediction method of UniVLA based on multi-head attention.
                Implementation details for VLA pretraining and finetuning are provided in Appendix~\ref{sec:vla_detail}.
              </p>
            </div>

            <div class="section">
              <p>
                Table~\ref{tab:simpler_res} shows that pretraining with MVP-LAM's latent actions improves downstream manipulation over other baselines.
                In particular, MVP-LAM increases the average success rate from 39.6\% (UniVLA) to 60.4\%, with gains on all four tasks.
                While LAPA achieves strong performance on some tasks, MVP-LAM remains competitive overall and yields the best average success rate.
              </p>
            </div>

            <div class="section">
              <p>
                Table~\ref{tab:libero_res} reports results on LIBERO-Long.
                MVP-LAM achieves 90.8\% success, improving over UniVLA pretrained on Bridge V2 (79.4\%).
                It also outperforms OpenVLA and $\pi_0$, and is comparable to UniVLA pretrained on OXE-scale.
              </p>
            </div>

            <div class="section">
              <p>
                Despite using a substantially smaller robot dataset for VLM pretraining ($\le$60k trajectories) than OXE-scale pretraining (typically $\ge$970k trajectories), MVP-LAM remains competitive on both SIMPLER and LIBERO-Long benchmarks.
                Notably, LIBERO-Long is used neither for VLM pretraining nor for LAM training, yet MVP-LAM attains a higher success rate.
                This improvement is consistent with the higher action-centricity of MVP-LAM latent actions (measured on Bridge V2 and LIBERO-Long).
                These results suggest that more action-centric latent actions provide a stronger pretraining signal and can translate into improved VLA finetuning performance.
              </p>
            </div>

            <h3>Does MVP-LAM Preserve Transition Information Under Viewpoint Perturbation</h3>
            <p>
              We evaluate whether LAMs preserve transition-relevant information under viewpoint perturbations.
              On Bridge V2, we construct 3.7k viewpoint-perturbed transitions using a NVS model.
              For original Bridge trajectory $\{I_t, a_t\}_{t=1}^T$, we construct viewpoint-perturbed trajectory $\{ \tilde{I}_t, a_t\}_{t=1}^T$ by synthesizing each image $I_t$ into $\tilde{I}_t$.
              Figure \ref{fig:nvs_res} shows an example of an original trajectory and its viewpoint-perturbed counterpart.
            </p>

            <div class="section">
              <p>
                Measuring $\mathcal{I}(Z_t; V_t, V_{t+1} \mid S_t, S_{t+1})$ requires viewpoint labels, which are not available in Bridge V2.
                We therefore use prediction error as an empirical proxy for how much viewpoint-dependent information remains in the latent action beyond the underlying state transition.
                We denote $o_t = f(I_t)$ and $\tilde{o}_t = f(\tilde{I}_t)$.
                Then, we extract latent actions from the original transition $(o_t, o_{t+1})$ and perturbed one $(o_t, \tilde{o}_{t+1})$, denoting them by $z_t$ and $\tilde{z}_t$, respectively.
                To standardize evaluation, we measure prediction errors in the DINOv2 feature space.
                We denote DINOv2 by $f_\text{dino}(\cdot)$, define $o_{t+1}^\text{dino}=f_\text{dino}(I_{t+1})$, and let $\hat{o}_{t+1}^\text{dino}(z)$ be the predicted next observation in the DINOv2 space from $z_t$.
                For LAMs that predict pixels, we embed decoded frames with $f_\text{dino}$.
              </p>
            </div>

            <div class="section">
              <p>
                $$\mathrm{MSE}=\left\lVert o_{t+1}^\text{dino}\! -\! \hat{o}_{t+1}^\text{dino}(z_t) \right\rVert_2^2, \quad
                \widetilde{\mathrm{MSE}}=\left\lVert o_{t+1}^\text{dino}\! -\! \hat{o}_{t+1}^\text{dino}(\tilde{z}_t) \right\rVert_2^2.$$
                Concretely, $\mathrm{MSE}$ uses $z_t$ from the original transition, whereas $\widetilde{\mathrm{MSE}}$ uses $\tilde{z}_t$ from the viewpoint-perturbed transition.
                Since $I_{t+1}$ and $\tilde{I}_{t+1}$ capture the same underlying state under different viewpoints, a larger $\widetilde{\mathrm{MSE}}$ suggests that the latent action is not purely determined by the state transition, but also depends on the viewpoint variation.
                This corresponds to $Z_t$ retaining more viewpoint-dependent factors beyond $(S_t,S_{t+1})$, which aligns with a larger $\mathcal{I}(Z_t; V_t, V_{t+1} \mid S_t, S_{t+1})$.
              </p>
            </div>

            <div class="section">
              <p>
                Beyond observation level reconstruction errors, we analyze the action-centricity of latent actions under the viewpoint variations.
                We report (i) the estimated mutual information $\hat{\mathcal{I}}(\tilde Z; A)$ computed from perturbed latent action $\tilde{Z}$, and (ii) NMSE for a linear probe trained on latent action from the original view and evaluated on latent actions from perturbed views.
                For MI estimation, we use KSG with same evaluation protocol in Section \ref{sec:mi_eval}, and do so for linear probing.
              </p>
            </div>

            <div class="section">
              <p>
                As shown in Figure~\ref{fig:nvs_res}, MVP-LAM attains the lowest $\mathrm{MSE}$ on the original sequences, which indicates accurate next observation prediction on unperturbed transitions.
                It also achieves the lowest $\widetilde{\mathrm{MSE}}$, which suggests that prediction accuracy is largely preserved even when the latent action is inferred from a viewpoint perturbed transition.
                In addition, MVP-LAM preserves the action centricity signals, with the highest KSG mutual information and the lowest cross view probing error, outperforming all baselines.
              </p>
            </div>

            <div class="section">
              <p>
                These results support the claim that MVP-LAM preserves transition relevant information under viewpoint perturbations.
                While the metrics in Figure~\ref{fig:nvs_res} are empirical proxies and do not directly estimate $\mathcal{I}(Z_t;V_t,V_{t+1}\mid S_t,S_{t+1})$, MVP-LAM consistently outperforms the baselines on $\widetilde{\mathrm{MSE}}$ and action-centricity, which is aligned with a reduction of viewpoint dependent information in the inferred latent action.
                We further provide qualitative and quantitative results for pixel based LAMs, which degrade substantially when conditioning the decoder on latent actions inferred from perturbed transitions, in Appendix~\ref{sec:robust_additional_analysis}.
              </p>
            </div>
          </div>

          <div class="section">
            <h2 id="tables">Tables</h2>

            <h3>SIMPLER benchmark result</h3>
            <p class="note">
              We report success rate and grasping rate (%) on the SIMPLER benchmark.
              $\dagger$ denotes results reported in prior work.
              Best is bolded and second best is underlined.
            </p>

            <div class="tablewrap">
              <table aria-label="SIMPLER benchmark result table">
                <thead>
                  <tr>
                    <th>Success Rate</th>
                    <th>MVP-LAM</th>
                    <th>UniVLA</th>
                    <th>LAPA$\dagger$</th>
                    <th>OpenVLA$\dagger$</th>
                    <th>Octo-Small</th>
                    <th>Octo-Base</th>
                    <th>$\pi_{0}$</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="strong">StackG2Y</td>
                    <td>33.3</td>
                    <td>16.7</td>
                    <td class="strong">54.2</td>
                    <td class="u">41.6</td>
                    <td>8.3</td>
                    <td>0.0</td>
                    <td>37.5</td>
                  </tr>
                  <tr>
                    <td class="strong">Carrot2Plate</td>
                    <td class="strong">66.7</td>
                    <td>20.8</td>
                    <td>45.8</td>
                    <td class="u">50.0</td>
                    <td>33.3</td>
                    <td>37.5</td>
                    <td>33.3</td>
                  </tr>
                  <tr>
                    <td class="strong">Spoon2Towel</td>
                    <td class="u">66.7</td>
                    <td>54.2</td>
                    <td class="strong">70.8</td>
                    <td>37.5</td>
                    <td>25.0</td>
                    <td>12.5</td>
                    <td>29.2</td>
                  </tr>
                  <tr>
                    <td class="strong">Eggplant2Bask</td>
                    <td class="strong">75.0</td>
                    <td class="u">66.7</td>
                    <td>58.3</td>
                    <td>16.7</td>
                    <td>12.5</td>
                    <td>20.8</td>
                    <td>45.8</td>
                  </tr>
                  <tr>
                    <td class="strong">AVG</td>
                    <td class="strong">60.4</td>
                    <td>39.6</td>
                    <td class="u">57.3</td>
                    <td>36.4</td>
                    <td>19.8</td>
                    <td>17.7</td>
                    <td>36.5</td>
                  </tr>

                  <tr>
                    <th>Grasping Rate</th>
                    <th colspan="7"></th>
                  </tr>

                  <tr>
                    <td class="strong">StackG2Y</td>
                    <td>54.3</td>
                    <td>45.8</td>
                    <td class="u">62.5</td>
                    <td>50.0</td>
                    <td>54.2</td>
                    <td class="strong">70.8</td>
                    <td>58.3</td>
                  </tr>
                  <tr>
                    <td class="strong">Carrot2Plate</td>
                    <td class="u">70.8</td>
                    <td>37.5</td>
                    <td>58.3</td>
                    <td>66.6</td>
                    <td class="strong">75.0</td>
                    <td>54.2</td>
                    <td>58.3</td>
                  </tr>
                  <tr>
                    <td class="strong">Spoon2Towel</td>
                    <td class="u">79.2</td>
                    <td class="u">79.2</td>
                    <td class="strong">83.3</td>
                    <td>45.8</td>
                    <td>66.7</td>
                    <td>70.8</td>
                    <td>54.2</td>
                  </tr>
                  <tr>
                    <td class="strong">Eggplant2Bask</td>
                    <td class="u">95.8</td>
                    <td class="strong">100.0</td>
                    <td>83.3</td>
                    <td>37.5</td>
                    <td>50.0</td>
                    <td>54.2</td>
                    <td>87.5</td>
                  </tr>
                  <tr>
                    <td class="strong">AVG</td>
                    <td class="strong">75.0</td>
                    <td>65.6</td>
                    <td class="u">71.9</td>
                    <td>50.0</td>
                    <td>61.5</td>
                    <td>62.5</td>
                    <td>64.6</td>
                  </tr>
                </tbody>
              </table>
            </div>

            <h3>LIBERO-Long results</h3>
            <p class="note">
              Success rate (%) on LIBERO-Long.
              $\dagger$ indicates results reported in prior work and $\ast$ indicates the methods that use additional wrist-view images and states.
              UniVLA (Bridge) uses a Bridge V2–pretrained VLM, while UniVLA (OXE) uses an OXE-pretrained VLM.
              Best is bolded and second best is underlined.
            </p>

            <div class="tablewrap">
              <table aria-label="LIBERO-Long results table">
                <thead>
                  <tr>
                    <th>MVP-LAM</th>
                    <th>UniVLA (Bridge)</th>
                    <th>OpenVLA $\dagger$</th>
                    <th>$\pi_0$ $\dagger\ast$</th>
                    <th>UniVLA$\dagger$ (OXE)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="strong">90.8</td>
                    <td class="u">79.4</td>
                    <td>53.7</td>
                    <td>85.2</td>
                    <td>92.0</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="section">
            <h2 id="bibtex">BibTeX</h2>
            <pre class="bibtex" id="bibtex-block" aria-label="bibtex">
@misc{lee2026mvplam,
  title  = {MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction},
  author = {Lee, Jung Min and Lee, Dohyeok and Ju, Seokhun and Cho, Taehyun and Koo, Jin Woo and Zhao, Li and Hong, Sangwoo and Lee, Jungwoo},
  year   = {2026},
  note   = {arXiv preprint, coming soon}
}
            </pre>
            <span class="sr-only" id="copy-status" aria-live="polite"></span>
          </div>
        </main>

        <aside class="card" id="figures">
          <h2>Figures</h2>

          <div class="figure">
            <img src="assets/concept.png" alt="Concept figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>Why viewpoint variation interferes with learning latent actions.</strong>
              Viewpoint variation acts as noise.
              Frame-to-frame visual differences reflect both interaction-driven state changes and viewpoint-dependent appearance changes (e.g., camera movements).
              Because these factors are entangled, the same underlying action can induce different visual transitions across viewpoints.
              This confounding makes it difficult to learn latent actions that are consistently predictive of the underlying control actions.
            </div>
          </div>

          <div class="figure">
            <img src="assets/arch.png" alt="Architecture figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>MVP-LAM training with time-synchronized multi-view videos.</strong>
              (1) <em>Self-viewpoint reconstruction</em> (left): for each view $v$, frozen DINOv2 extracts features $(o_t^v,o_{t+1}^v)$.
              A spatiotemporal encoder produces a continuous latent $e_t^v$ that is vector-quantized into a discrete token $z_t^v$, and a decoder reconstructs $o_{t+1}^v$ from $(o_t^v,z_t^v)$.
              (2) <em>Cross-viewpoint reconstruction</em> (right): MVP-LAM swaps latent tokens across views (e.g., $z_t^{v_1}\leftrightarrow z_t^{v_2}$) while reconstructing each view’s future feature, encouraging $z_t$ to capture inherent transition information.
            </div>
          </div>

          <div class="figure">
            <img src="assets/mi.png" alt="Mutual information figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>Estimated mutual information.</strong>
              $\mathcal{I}(Z;A)$ on Bridge V2 with KSG, BA, and MINE estimators.
              For KSG, latent actions are randomly projected to $d{=}256$ prior to estimation.
              Higher is better.
              Error bars show standard deviation over four seeds.
            </div>
          </div>

          <div class="figure">
            <img src="assets/nmse_probe.png" alt="Linear probing figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>Linear probing result.</strong>
              NMSE of a linear layer predicting actions from latent actions.
              Bridge V2 is in-distribution; LIBERO (Spatial/Object/Goal/Long) is out-of-distribution.
              Lower is better.
              Error bars show standard deviation over four seeds.
            </div>
          </div>

          <div class="figure">
            <img src="assets/pretrain_finetune_detail.png" alt="Pretrain and finetune overview figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>Overview of the VLM pretraining and VLA finetuning with example demonstrations.</strong>
              Left: sample observation sequences from SIMPLER and LIBERO-Long with natural language goal description.
              Right: <strong>(1) VLM Pretraining.</strong>
              Prismatic-7B VLM is pretrained to predict the discrete latent action token, which is produced by MVP-LAM, from an image and language instruction using a CE loss.
              <strong>(2) VLA Finetuning.</strong>
              VLA initializes from the pretrained VLM and finetunes on downstream demonstrations to predict robot actions.
            </div>
          </div>

          <div class="figure">
            <img src="assets/nvs_res_traj.png" alt="Viewpoint perturbed trajectory figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>Viewpoint perturbed trajectory and evaluation.</strong>
              (Left): an example trajectory from the original camera view $\{I_t\}$ (top) and its viewpoint perturbed counterpart $\{\tilde{I}_t\}$ (bottom).
              (Right): reconstruction error on the original and perturbed sequences (top), reporting $\mathrm{MSE}$ and $\widetilde{\mathrm{MSE}}$ and action-centricity metrics (bottom), reporting KSG mutual information and NMSE of linear probing, for MVP-LAM and baselines.
              Error bars show standard deviation over 3 random seeds.
            </div>
          </div>

          <div class="note">
            Put images in the <code>assets</code> folder with filenames shown above
          </div>
        </aside>
      </div>
    </div>

    <script>
      (function () {
        var yearEl = document.getElementById("year");
        yearEl.textContent = String(new Date().getFullYear());

        var copyBtn = document.getElementById("copy-bib");
        var bib = document.getElementById("bibtex-block");
        var status = document.getElementById("copy-status");

        function setStatus(msg) {
          status.textContent = msg;
          setTimeout(function () { status.textContent = ""; }, 1200);
        }

        copyBtn.addEventListener("click", async function () {
          var text = bib.textContent.trim();
          try {
            await navigator.clipboard.writeText(text);
            setStatus("BibTeX copied");
          } catch (e) {
            var ta = document.createElement("textarea");
            ta.value = text;
            document.body.appendChild(ta);
            ta.select();
            document.execCommand("copy");
            document.body.removeChild(ta);
            setStatus("BibTeX copied");
          }
        });
      })();
    </script>
  </body>
</html>
