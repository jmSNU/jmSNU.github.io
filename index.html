<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>MVP-LAM | Cross-Viewpoint Reconstruction</title>
    <meta
      name="description"
      content="MVP-LAM learns action-centric discrete latent actions from time-synchronized multi-view videos via cross-viewpoint reconstruction."
    />

    <meta property="og:title" content="MVP-LAM" />
    <meta
      property="og:description"
      content="Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction"
    />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://latentactionpretraining.github.io/" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />

    <style>
      :root {
        --bg: #0b0c10;
        --panel: #111318;
        --text: #e9eef5;
        --muted: #aab4c3;
        --line: rgba(255, 255, 255, 0.10);
        --accent: #7aa2ff;
        --accent2: #a6ffcb;
        --shadow: 0 10px 30px rgba(0, 0, 0, 0.40);
      }

      * { box-sizing: border-box; }
      html, body { height: 100%; }
      body {
        margin: 0;
        font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
        background:
          radial-gradient(1200px 700px at 20% 0%, rgba(122, 162, 255, 0.22), transparent 60%),
          radial-gradient(900px 600px at 85% 15%, rgba(166, 255, 203, 0.12), transparent 55%),
          var(--bg);
        color: var(--text);
        line-height: 1.55;
      }

      a { color: inherit; text-decoration: none; }
      a:hover { text-decoration: underline; }

      .wrap {
        width: min(1100px, calc(100% - 40px));
        margin: 0 auto;
        padding: 26px 0 64px;
      }

      header {
        display: grid;
        gap: 16px;
        padding: 22px 0 10px;
      }

      .topbar {
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 12px;
        flex-wrap: wrap;
      }

      .badge {
        display: inline-flex;
        align-items: center;
        gap: 10px;
        padding: 8px 12px;
        border: 1px solid var(--line);
        border-radius: 999px;
        background: rgba(255, 255, 255, 0.03);
        width: fit-content;
      }
      .dot {
        width: 9px;
        height: 9px;
        border-radius: 999px;
        background: var(--accent);
        box-shadow: 0 0 0 6px rgba(122, 162, 255, 0.15);
      }
      .badge span {
        color: var(--muted);
        font-size: 13px;
        font-weight: 500;
      }

      .nav {
        display: flex;
        gap: 10px;
        flex-wrap: wrap;
      }
      .nav a {
        font-size: 13px;
        color: var(--muted);
        border: 1px solid var(--line);
        background: rgba(255, 255, 255, 0.02);
        padding: 7px 10px;
        border-radius: 999px;
      }
      .nav a:hover { color: var(--text); text-decoration: none; }

      h1 {
        margin: 0;
        font-size: clamp(28px, 4vw, 44px);
        letter-spacing: -0.02em;
        line-height: 1.15;
      }
      .subtitle {
        margin: 0;
        color: var(--muted);
        font-size: clamp(15px, 2vw, 18px);
        max-width: 95ch;
      }

      .meta {
        display: grid;
        gap: 8px;
        margin-top: 2px;
      }
      .authors {
        color: var(--text);
        font-weight: 600;
        flex-wrap: wrap;
      }
      .affils {
        color: var(--muted);
        font-size: 14px;
      }

      .cta {
        display: flex;
        flex-wrap: wrap;
        gap: 10px;
        margin-top: 10px;
      }
      .btn {
        display: inline-flex;
        align-items: center;
        gap: 10px;
        padding: 10px 14px;
        border-radius: 12px;
        border: 1px solid var(--line);
        background: rgba(255, 255, 255, 0.03);
        box-shadow: var(--shadow);
        font-weight: 600;
        font-size: 14px;
        cursor: pointer;
        user-select: none;
      }
      .btn:hover { background: rgba(255, 255, 255, 0.06); text-decoration: none; }
      .btn.primary {
        border-color: rgba(122, 162, 255, 0.50);
        background: linear-gradient(180deg, rgba(122, 162, 255, 0.20), rgba(122, 162, 255, 0.06));
      }
      .btn.disabled {
        opacity: 0.55;
        cursor: not-allowed;
        box-shadow: none;
      }
      .btn .pill {
        padding: 3px 8px;
        border-radius: 999px;
        border: 1px solid var(--line);
        color: var(--muted);
        font-weight: 600;
        font-size: 12px;
      }

      .layout {
        display: grid;
        grid-template-columns: 1.35fr 0.65fr;
        gap: 18px;
        margin-top: 16px;
      }
      @media (max-width: 980px) {
        .layout { grid-template-columns: 1fr; }
      }

      .card {
        background: rgba(255, 255, 255, 0.03);
        border: 1px solid var(--line);
        border-radius: 16px;
        box-shadow: var(--shadow);
        padding: 18px;
      }

      .card h2 {
        margin: 0 0 10px;
        font-size: 16px;
        letter-spacing: -0.01em;
      }

      .card h3 {
        margin: 18px 0 8px;
        font-size: 14px;
        letter-spacing: -0.01em;
        color: var(--text);
      }

      .card p {
        margin: 0;
        color: var(--muted);
        font-size: 14px;
      }

      .section { margin-top: 18px; }

      .figure {
        display: grid;
        gap: 10px;
        margin-top: 12px;
      }
      .figure img {
        width: 100%;
        height: auto;
        border-radius: 14px;
        border: 1px solid var(--line);
        background: rgba(255, 255, 255, 0.02);
      }
      .figcap {
        color: var(--muted);
        font-size: 13px;
      }

      .tablewrap {
        margin-top: 10px;
        overflow: auto;
        border: 1px solid var(--line);
        border-radius: 12px;
        background: rgba(255, 255, 255, 0.02);
      }
      table {
        width: 100%;
        border-collapse: collapse;
        font-size: 13px;
        min-width: 780px;
      }
      th, td {
        padding: 10px 10px;
        border-bottom: 1px solid var(--line);
        text-align: left;
        vertical-align: middle;
        white-space: nowrap;
      }
      th {
        color: var(--text);
        font-weight: 700;
        background: rgba(255, 255, 255, 0.03);
      }
      td { color: var(--muted); }
      tr:last-child td { border-bottom: none; }

      .strong { color: var(--text); font-weight: 700; }
      .u { text-decoration: underline; text-decoration-thickness: 1px; text-underline-offset: 3px; }

      .bibtex {
        margin-top: 10px;
        background: #0a0b0f;
        border: 1px solid var(--line);
        border-radius: 12px;
        padding: 12px;
        overflow: auto;
        font-size: 12px;
        color: #dfe7ff;
      }

      .note {
        margin-top: 10px;
        padding: 10px 12px;
        border: 1px solid var(--line);
        border-radius: 12px;
        background: rgba(255, 255, 255, 0.02);
        color: var(--muted);
        font-size: 13px;
      }

      footer {
        margin-top: 22px;
        color: var(--muted);
        font-size: 12px;
        display: flex;
        justify-content: space-between;
        flex-wrap: wrap;
        gap: 10px;
        border-top: 1px solid var(--line);
        padding-top: 16px;
      }

      .smalllink { color: var(--muted); }
      .smalllink:hover { color: var(--text); }

      .sr-only {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0,0,0,0);
        white-space: nowrap;
        border: 0;
      }

      code {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        font-size: 12px;
        color: #dfe7ff;
      }
    </style>

    <script>
      window.MathJax = {
        tex: { inlineMath: [["$", "$"], ["\\(", "\\)"]] },
        options: { skipHtmlTags: ["script","noscript","style","textarea","pre","code"] }
      };
    </script>
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>

  <body>
    <div class="wrap">
      <header>
        <div class="topbar">
          <div class="badge" aria-label="project status">
            <div class="dot" aria-hidden="true"></div>
            <span>ArXiv Preprint</span>
          </div>

          <nav class="nav" aria-label="page">
            <a href="#abstract">Abstract</a>
            <a href="#introduction">Introduction</a>
            <a href="#method">Method</a>
            <a href="#experiments">Experiments</a>
            <a href="#tables">Tables</a>
            <a href="#figures">Figures</a>
            <a href="#bibtex">BibTeX</a>
          </nav>
        </div>

        <h1>MVP-LAM</h1>
        <p class="subtitle">
          Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction
        </p>

        <div class="meta">
          <div class="authors">
            Jung Min Lee · Dohyeok Lee · Seokhun Ju · Taehyun Cho · Jin Woo Koo · Li Zhao · Sangwoo Hong · Jungwoo Lee
          </div>
          <div class="affils">
            Seoul National University · Konkuk University · Microsoft Research Asia · HodooAI Labs
          </div>
        </div>

        <div class="cta" role="group" aria-label="links">
          <a class="btn primary disabled" href="#" aria-disabled="true" tabindex="-1">
            <span>arXiv</span>
            <span class="pill">Coming soon</span>
          </a>
          <a class="btn disabled" href="#" aria-disabled="true" tabindex="-1">
            <span>Code</span>
            <span class="pill">Coming soon</span>
          </a>
          <button class="btn" id="copy-bib" type="button">
            <span>Copy BibTeX</span>
            <span class="pill">1 click</span>
          </button>
        </div>
      </header>

      <div class="layout">
        <main class="card">
          <h2 id="abstract">Abstract</h2>
          <p>
            Learning <em>latent actions</em> from diverse human videos enables scaling robot learning beyond embodiment-specific robot datasets, and these latent actions have recently been used as pseudo-action labels for vision-language-action (VLA) model pretraining.
            To make VLA pretraining effective, latent actions should contain information about the underlying agent's actions despite the absence of ground-truth labels.
            We propose <strong>M</strong>ulti-<strong>V</strong>iew<strong>P</strong>oint <strong>L</strong>atent <strong>A</strong>ction <strong>M</strong>odel (<strong>MVP-LAM</strong>), which learns discrete latent actions that are highly informative about ground-truth actions from time-synchronized multi-view videos.
            MVP-LAM trains latent actions with a <em>cross-viewpoint reconstruction</em> objective, so that a latent action inferred from one view must explain the future in another view, reducing reliance on viewpoint-specific cues.
            On Bridge V2, MVP-LAM produces more action-centric latent actions, achieving higher mutual information with ground-truth actions and improved action prediction, including under out-of-distribution evaluation.
            Finally, pretraining VLAs with MVP-LAM latent actions improves downstream manipulation performance on the SIMPLER and LIBERO-Long benchmarks.
          </p>

          <div class="section">
            <h2 id="method">Method</h2>

            <h3>Action-centric Latent Action</h3>
            <p>
              When latent actions are used as pseudo-action labels for behavior cloning policies, it is desirable that the learned latent action $Z_t$ preserves as much information as possible about the underlying action $A_t$.
              We denote the state by $S_t$, and assume an expert policy induces actions $A_t \sim \pi^\star(\cdot \mid S_t)$ for a given task.
              In the pretraining stage, we typically do not observe $S_t$ or $A_t$.
              Instead, we only observe images (or their features) $O_t = f(I_t)$.
              LAM produces latent actions from consecutive observations, i.e., $Z_t = E_\theta(O_t, O_{t+1})$ (with vector quantization when using VQ-VAE).
            </p>

            <div class="section">
              <p>
                Motivated by \citet{latentaction_learn}, we define a latent action $Z_t$ <em>action-centric</em> if it is highly informative about the underlying action $A_t$.
                We quantify this by mutual information and consider the objective
                $$\max_{Z_t} \ \mathcal{I}(Z_t; A_t).$$
                In this context, viewpoint variation acts as noise.
                Changes in camera pose $V_t$ can induce frame-to-frame differences in $O_t$ that are predictive of $Z_t$ but are not caused by the action $A_t$.
                When $Z_t$ is learned under a limited-capacity bottleneck such as vector quantization, allocating representational capacity to viewpoint-dependent factors can come at the expense of action-relevant dynamics and reduce $\mathcal{I}(Z_t;A_t)$.
                Under simplifying assumptions detailed in Appendix~\ref{sec:mi_analysis}, one can derive a lower bound
                $$\mathcal{I}(Z_t;A_t) \ge \mathcal{H}(Z_t) - \mathcal{I}(Z_t;V_t,V_{t+1}\mid S_t,S_{t+1}) - C$$
                where $C$ is a constant independent of the latent action $Z_t$.
                This bound suggests that when $\mathcal{H}(Z_t)$ is constrained, decreasing $\mathcal{I}(Z_t;V_t,V_{t+1}\mid S_t,S_{t+1})$ can improve action-centricity.
                This motivates using time-synchronized multi-view videos together with a cross-viewpoint reconstruction objective to discourage viewpoint-dependent factors in $Z_t$.
              </p>
            </div>

            <h3>Multi-Viewpoint Latent Action Learning</h3>
            <p>
              Building on this motivation, we introduce MVP-LAM, which leverages time-synchronized multi-view videos and cross-viewpoint reconstruction to learn action-centric latent actions.
              Although single-view capture is more convenient to collect than multi-view, it remains practical at scale for human videos \cite{tcn}, and various multi-view human datasets are readily available \cite{h2o, havid, assembly101, egoexo4d}.
              For clarity, we describe the two-view case but note that the objective extends to more views.
            </p>

          </div>

          <div class="section">
            <h2 id="experiments">Experiments</h2>

            <h3> <strong>RQ1.</strong> Are MVP-LAM latent actions more action-centric</h3>
            <p>
              We evaluate how action-centric a latent action is by measuring (i) mutual information between latent actions and ground-truth actions, and (ii) how well actions can be predicted from latent actions with a simple linear layer.
            </p>

            <div class="section">
              <p>
                On Bridge V2, we estimate $\mathcal{I}(Z;A)$ using three estimators.
                the nonparametric Kraskov--St\"ogbauer--Grassberger (KSG) estimator, and two variational estimators (Barber--Agakov (BA)~\cite{ba} and a MINE style bound~\cite{mine}).
                We use $k{=}5$ for KSG.
                Since KSG is unstable in high dimensions, we apply a random projection to the latent actions so that the overall latent action dimension, including the code length, becomes $d{=}256$ before KSG.
                We provide details of MI evaluation in Appendix \ref{sec:action_centric_detail}.
              </p>
            </div>

            <div class="section">
              <p>
                As shown in Figure \ref{fig:mi}, MVP-LAM achieves the highest estimated $\mathcal{I}(Z;A)$ across all estimators, suggesting that its latent actions preserve more information about the actions than the baselines.
                Consistent with MI estimation, Figure \ref{fig:nmse_probe} shows that MVP-LAM achieves lower NMSE on Bridge V2 and on OOD LIBERO suites (Spatial, Object, and Long), with a small drop on LIBERO-Goal relative to UniVLA.
                Overall, MI estimation and probing consistently indicate that MVP-LAM learns more action-centric latent actions.
                We note that UniVLA may struggle to achieve action-centricity because its training objective is primarily driven by task information from language descriptions, which are typically trajectory-level, and this provides weaker supervision for encoding step-level action signals in $z_t$.
                The details of linear probing and extended analysis including LAPA and Moto are listed in Appendix \ref{sec:action_centric_detail}.
              </p>
            </div>

            <h3> <strong>RQ2.</strong> Is MVP-LAM Effective for Manipulation</h3>

            <div class="section">
              <p>
                Table~\ref{tab:simpler_res} shows that pretraining with MVP-LAM's latent actions improves downstream manipulation over other baselines.
                In particular, MVP-LAM increases the average success rate from 39.6\% (UniVLA) to 60.4\%, with gains on all four tasks.
                While LAPA achieves strong performance on some tasks, MVP-LAM remains competitive overall and yields the best average success rate.
              </p>
            </div>

            <div class="section">
              <p>
                Table~\ref{tab:libero_res} reports results on LIBERO-Long.
                MVP-LAM achieves 90.8\% success, improving over UniVLA pretrained on Bridge V2 (79.4\%).
                It also outperforms OpenVLA and $\pi_0$, and is comparable to UniVLA pretrained on OXE-scale.
              </p>
            </div>

            <div class="section">
              <p>
                Despite using a substantially smaller robot dataset for VLM pretraining ($\le$60k trajectories) than OXE-scale pretraining (typically $\ge$970k trajectories), MVP-LAM remains competitive on both SIMPLER and LIBERO-Long benchmarks.
                Notably, LIBERO-Long is used neither for VLM pretraining nor for LAM training, yet MVP-LAM attains a higher success rate.
                This improvement is consistent with the higher action-centricity of MVP-LAM latent actions (measured on Bridge V2 and LIBERO-Long).
                These results suggest that more action-centric latent actions provide a stronger pretraining signal and can translate into improved VLA finetuning performance.
              </p>
            </div>

            <h3> <strong>RQ3.</strong> Does MVP-LAM Preserve Transition Information Under Viewpoint Perturbation</h3>
            <div class="section">
              <p>
                $$\mathrm{MSE}=\left\lVert o_{t+1}^\text{dino}\! -\! \hat{o}_{t+1}^\text{dino}(z_t) \right\rVert_2^2, \quad
                \widetilde{\mathrm{MSE}}=\left\lVert o_{t+1}^\text{dino}\! -\! \hat{o}_{t+1}^\text{dino}(\tilde{z}_t) \right\rVert_2^2.$$
                Concretely, $\mathrm{MSE}$ uses $z_t$ from the original transition, whereas $\widetilde{\mathrm{MSE}}$ uses $\tilde{z}_t$ from the viewpoint-perturbed transition.
                Since $I_{t+1}$ and $\tilde{I}_{t+1}$ capture the same underlying state under different viewpoints, a larger $\widetilde{\mathrm{MSE}}$ suggests that the latent action is not purely determined by the state transition, but also depends on the viewpoint variation.
                This corresponds to $Z_t$ retaining more viewpoint-dependent factors beyond $(S_t,S_{t+1})$, which aligns with a larger $\mathcal{I}(Z_t; V_t, V_{t+1} \mid S_t, S_{t+1})$.
              </p>
            </div>

            <div class="section">
              <p>
                Beyond observation level reconstruction errors, we analyze the action-centricity of latent actions under the viewpoint variations.
                We report (i) the estimated mutual information $\hat{\mathcal{I}}(\tilde Z; A)$ computed from perturbed latent action $\tilde{Z}$, and (ii) NMSE for a linear probe trained on latent action from the original view and evaluated on latent actions from perturbed views.
                For MI estimation, we use KSG with same evaluation protocol in Section \ref{sec:mi_eval}, and do so for linear probing.
              </p>
            </div>

            <div class="section">
              <p>
                As shown in Figure~\ref{fig:nvs_res}, MVP-LAM attains the lowest $\mathrm{MSE}$ on the original sequences, which indicates accurate next observation prediction on unperturbed transitions.
                It also achieves the lowest $\widetilde{\mathrm{MSE}}$, which suggests that prediction accuracy is largely preserved even when the latent action is inferred from a viewpoint perturbed transition.
                In addition, MVP-LAM preserves the action centricity signals, with the highest KSG mutual information and the lowest cross view probing error, outperforming all baselines.
              </p>
            </div>

            <div class="section">
              <p>
                These results support the claim that MVP-LAM preserves transition relevant information under viewpoint perturbations.
                While the metrics in Figure~\ref{fig:nvs_res} are empirical proxies and do not directly estimate $\mathcal{I}(Z_t;V_t,V_{t+1}\mid S_t,S_{t+1})$, MVP-LAM consistently outperforms the baselines on $\widetilde{\mathrm{MSE}}$ and action-centricity, which is aligned with a reduction of viewpoint dependent information in the inferred latent action.
                We further provide qualitative and quantitative results for pixel based LAMs, which degrade substantially when conditioning the decoder on latent actions inferred from perturbed transitions, in Appendix~\ref{sec:robust_additional_analysis}.
              </p>
            </div>
          </div>

          <div class="section">
            <h2 id="tables">Tables</h2>

            <h3>SIMPLER benchmark result</h3>
            <p class="note">
              We report success rate and grasping rate (%) on the SIMPLER benchmark.
              $\dagger$ denotes results reported in prior work.
              Best is bolded and second best is underlined.
            </p>

            <div class="tablewrap">
              <table aria-label="SIMPLER benchmark result table">
                <thead>
                  <tr>
                    <th>Success Rate</th>
                    <th>MVP-LAM</th>
                    <th>UniVLA</th>
                    <th>LAPA$\dagger$</th>
                    <th>OpenVLA$\dagger$</th>
                    <th>Octo-Small</th>
                    <th>Octo-Base</th>
                    <th>$\pi_{0}$</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="strong">StackG2Y</td>
                    <td>33.3</td>
                    <td>16.7</td>
                    <td class="strong">54.2</td>
                    <td class="u">41.6</td>
                    <td>8.3</td>
                    <td>0.0</td>
                    <td>37.5</td>
                  </tr>
                  <tr>
                    <td class="strong">Carrot2Plate</td>
                    <td class="strong">66.7</td>
                    <td>20.8</td>
                    <td>45.8</td>
                    <td class="u">50.0</td>
                    <td>33.3</td>
                    <td>37.5</td>
                    <td>33.3</td>
                  </tr>
                  <tr>
                    <td class="strong">Spoon2Towel</td>
                    <td class="u">66.7</td>
                    <td>54.2</td>
                    <td class="strong">70.8</td>
                    <td>37.5</td>
                    <td>25.0</td>
                    <td>12.5</td>
                    <td>29.2</td>
                  </tr>
                  <tr>
                    <td class="strong">Eggplant2Bask</td>
                    <td class="strong">75.0</td>
                    <td class="u">66.7</td>
                    <td>58.3</td>
                    <td>16.7</td>
                    <td>12.5</td>
                    <td>20.8</td>
                    <td>45.8</td>
                  </tr>
                  <tr>
                    <td class="strong">AVG</td>
                    <td class="strong">60.4</td>
                    <td>39.6</td>
                    <td class="u">57.3</td>
                    <td>36.4</td>
                    <td>19.8</td>
                    <td>17.7</td>
                    <td>36.5</td>
                  </tr>

                  <tr>
                    <th>Grasping Rate</th>
                    <th colspan="7"></th>
                  </tr>

                  <tr>
                    <td class="strong">StackG2Y</td>
                    <td>54.3</td>
                    <td>45.8</td>
                    <td class="u">62.5</td>
                    <td>50.0</td>
                    <td>54.2</td>
                    <td class="strong">70.8</td>
                    <td>58.3</td>
                  </tr>
                  <tr>
                    <td class="strong">Carrot2Plate</td>
                    <td class="u">70.8</td>
                    <td>37.5</td>
                    <td>58.3</td>
                    <td>66.6</td>
                    <td class="strong">75.0</td>
                    <td>54.2</td>
                    <td>58.3</td>
                  </tr>
                  <tr>
                    <td class="strong">Spoon2Towel</td>
                    <td class="u">79.2</td>
                    <td class="u">79.2</td>
                    <td class="strong">83.3</td>
                    <td>45.8</td>
                    <td>66.7</td>
                    <td>70.8</td>
                    <td>54.2</td>
                  </tr>
                  <tr>
                    <td class="strong">Eggplant2Bask</td>
                    <td class="u">95.8</td>
                    <td class="strong">100.0</td>
                    <td>83.3</td>
                    <td>37.5</td>
                    <td>50.0</td>
                    <td>54.2</td>
                    <td>87.5</td>
                  </tr>
                  <tr>
                    <td class="strong">AVG</td>
                    <td class="strong">75.0</td>
                    <td>65.6</td>
                    <td class="u">71.9</td>
                    <td>50.0</td>
                    <td>61.5</td>
                    <td>62.5</td>
                    <td>64.6</td>
                  </tr>
                </tbody>
              </table>
            </div>

            <h3>LIBERO-Long results</h3>
            <p class="note">
              Success rate (%) on LIBERO-Long.
              $\dagger$ indicates results reported in prior work and $\ast$ indicates the methods that use additional wrist-view images and states.
              UniVLA (Bridge) uses a Bridge V2–pretrained VLM, while UniVLA (OXE) uses an OXE-pretrained VLM.
              Best is bolded and second best is underlined.
            </p>

            <div class="tablewrap">
              <table aria-label="LIBERO-Long results table">
                <thead>
                  <tr>
                    <th>MVP-LAM</th>
                    <th>UniVLA (Bridge)</th>
                    <th>OpenVLA $\dagger$</th>
                    <th>$\pi_0$ $\dagger\ast$</th>
                    <th>UniVLA$\dagger$ (OXE)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="strong">90.8</td>
                    <td class="u">79.4</td>
                    <td>53.7</td>
                    <td>85.2</td>
                    <td>92.0</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="section">
            <h2 id="bibtex">BibTeX</h2>
            <pre class="bibtex" id="bibtex-block" aria-label="bibtex">
@misc{lee2026mvplam,
  title  = {MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction},
  author = {Lee, Jung Min and Lee, Dohyeok and Ju, Seokhun and Cho, Taehyun and Koo, Jin Woo and Zhao, Li and Hong, Sangwoo and Lee, Jungwoo},
  year   = {2026},
  note   = {arXiv preprint, coming soon}
}
            </pre>
            <span class="sr-only" id="copy-status" aria-live="polite"></span>
          </div>
        </main>

        <aside class="card" id="figures">
          <h2>Figures</h2>

          <div class="figure">
            <img src="assets/concept.png" alt="Concept figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>Why viewpoint variation interferes with learning latent actions.</strong>
              Viewpoint variation acts as noise.
              Frame-to-frame visual differences reflect both interaction-driven state changes and viewpoint-dependent appearance changes (e.g., camera movements).
              Because these factors are entangled, the same underlying action can induce different visual transitions across viewpoints.
              This confounding makes it difficult to learn latent actions that are consistently predictive of the underlying control actions.
            </div>
          </div>

          <div class="figure">
            <img src="assets/arch.png" alt="Architecture figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>MVP-LAM training with time-synchronized multi-view videos.</strong>
              (1) <em>Self-viewpoint reconstruction</em> (left): for each view $v$, frozen DINOv2 extracts features $(o_t^v,o_{t+1}^v)$.
              A spatiotemporal encoder produces a continuous latent $e_t^v$ that is vector-quantized into a discrete token $z_t^v$, and a decoder reconstructs $o_{t+1}^v$ from $(o_t^v,z_t^v)$.
              (2) <em>Cross-viewpoint reconstruction</em> (right): MVP-LAM swaps latent tokens across views (e.g., $z_t^{v_1}\leftrightarrow z_t^{v_2}$) while reconstructing each view’s future feature, encouraging $z_t$ to capture inherent transition information.
            </div>
          </div>

          <div class="figure">
            <img src="assets/mi.png" alt="Mutual information figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>Estimated mutual information.</strong>
              $\mathcal{I}(Z;A)$ on Bridge V2 with KSG, BA, and MINE estimators.
              For KSG, latent actions are randomly projected to $d{=}256$ prior to estimation.
              Higher is better.
              Error bars show standard deviation over four seeds.
            </div>
          </div>

          <div class="figure">
            <img src="assets/nmse_bridge_libero_clean_all_oneplot.png" alt="Linear probing figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>Linear probing result.</strong>
              NMSE of a linear layer predicting actions from latent actions.
              Bridge V2 is in-distribution; LIBERO (Spatial/Object/Goal/Long) is out-of-distribution.
              Lower is better.
              Error bars show standard deviation over four seeds.
            </div>
          </div>

          <div class="figure">
            <img src="assets/nvs_res_traj.png" alt="Viewpoint perturbed trajectory figure" onerror="this.style.display='none'" />
            <div class="figcap">
              <strong>Viewpoint perturbed trajectory and evaluation.</strong>
              (Left): an example trajectory from the original camera view $\{I_t\}$ (top) and its viewpoint perturbed counterpart $\{\tilde{I}_t\}$ (bottom).
              (Right): reconstruction error on the original and perturbed sequences (top), reporting $\mathrm{MSE}$ and $\widetilde{\mathrm{MSE}}$ and action-centricity metrics (bottom), reporting KSG mutual information and NMSE of linear probing, for MVP-LAM and baselines.
              Error bars show standard deviation over 3 random seeds.
            </div>
          </div>

          <div class="note">
            Put images in the <code>assets</code> folder with filenames shown above
          </div>
        </aside>
      </div>
    </div>

    <script>
      (function () {
        var yearEl = document.getElementById("year");
        yearEl.textContent = String(new Date().getFullYear());

        var copyBtn = document.getElementById("copy-bib");
        var bib = document.getElementById("bibtex-block");
        var status = document.getElementById("copy-status");

        function setStatus(msg) {
          status.textContent = msg;
          setTimeout(function () { status.textContent = ""; }, 1200);
        }

        copyBtn.addEventListener("click", async function () {
          var text = bib.textContent.trim();
          try {
            await navigator.clipboard.writeText(text);
            setStatus("BibTeX copied");
          } catch (e) {
            var ta = document.createElement("textarea");
            ta.value = text;
            document.body.appendChild(ta);
            ta.select();
            document.execCommand("copy");
            document.body.removeChild(ta);
            setStatus("BibTeX copied");
          }
        });
      })();
    </script>
  </body>
</html>
