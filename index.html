<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>MVP-LAM | Cross-Viewpoint Reconstruction</title>
    <meta
      name="description"
      content="MVP-LAM learns action-centric discrete latent actions from time-synchronized multi-view videos via cross-viewpoint reconstruction."
    />

    <meta property="og:title" content="MVP-LAM" />
    <meta
      property="og:description"
      content="Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction"
    />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://latentactionpretraining.github.io/" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />

    <style>
      :root {
        --bg: #0b0c10;
        --panel: #111318;
        --text: #e9eef5;
        --muted: #aab4c3;
        --line: rgba(255, 255, 255, 0.10);
        --accent: #7aa2ff;
        --accent2: #a6ffcb;
        --shadow: 0 10px 30px rgba(0, 0, 0, 0.40);
      }

      * { box-sizing: border-box; }
      html, body { height: 100%; }
      body {
        margin: 0;
        font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
        background:
          radial-gradient(1200px 700px at 20% 0%, rgba(122, 162, 255, 0.22), transparent 60%),
          radial-gradient(900px 600px at 85% 15%, rgba(166, 255, 203, 0.12), transparent 55%),
          var(--bg);
        color: var(--text);
        line-height: 1.55;
      }

      a { color: inherit; text-decoration: none; }
      a:hover { text-decoration: underline; }

      .wrap {
        width: min(1100px, calc(100% - 40px));
        margin: 0 auto;
        padding: 26px 0 64px;
      }

      header {
        display: grid;
        gap: 16px;
        padding: 22px 0 10px;
      }

      .topbar {
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 12px;
        flex-wrap: wrap;
      }

      .badge {
        display: inline-flex;
        align-items: center;
        gap: 10px;
        padding: 8px 12px;
        border: 1px solid var(--line);
        border-radius: 999px;
        background: rgba(255, 255, 255, 0.03);
        width: fit-content;
      }
      .dot {
        width: 9px;
        height: 9px;
        border-radius: 999px;
        background: var(--accent);
        box-shadow: 0 0 0 6px rgba(122, 162, 255, 0.15);
      }
      .badge span {
        color: var(--muted);
        font-size: 13px;
        font-weight: 500;
      }

      .nav {
        display: flex;
        gap: 10px;
        flex-wrap: wrap;
      }
      .nav a {
        font-size: 13px;
        color: var(--muted);
        border: 1px solid var(--line);
        background: rgba(255, 255, 255, 0.02);
        padding: 7px 10px;
        border-radius: 999px;
      }
      .nav a:hover { color: var(--text); text-decoration: none; }

      h1 {
        margin: 0;
        font-size: clamp(28px, 4vw, 44px);
        letter-spacing: -0.02em;
        line-height: 1.15;
      }
      .subtitle {
        margin: 0;
        color: var(--muted);
        font-size: clamp(15px, 2vw, 18px);
        max-width: 95ch;
      }

      .meta {
        display: grid;
        gap: 8px;
        margin-top: 2px;
      }

      .authors {
        color: var(--text);
        font-weight: 600;
        flex-wrap: wrap;
        font-size: 14px;
      }
      .authors a {
        color: var(--text);
        text-decoration: none;
        border-bottom: 1px solid rgba(255,255,255,0.15);
      }
      .authors a:hover {
        text-decoration: none;
        border-bottom-color: rgba(255,255,255,0.45);
      }

      .affils {
        color: var(--muted);
        font-size: 14px;
      }

      .cta {
        display: flex;
        flex-wrap: wrap;
        gap: 10px;
        margin-top: 10px;
      }
      .btn {
        display: inline-flex;
        align-items: center;
        gap: 10px;
        padding: 10px 14px;
        border-radius: 12px;
        border: 1px solid var(--line);
        background: rgba(255, 255, 255, 0.03);
        box-shadow: var(--shadow);
        font-weight: 600;
        font-size: 14px;
        cursor: pointer;
        user-select: none;
      }
      .btn:hover { background: rgba(255, 255, 255, 0.06); text-decoration: none; }
      .btn.primary {
        border-color: rgba(122, 162, 255, 0.50);
        background: linear-gradient(180deg, rgba(122, 162, 255, 0.20), rgba(122, 162, 255, 0.06));
      }
      .btn.disabled {
        opacity: 0.55;
        cursor: not-allowed;
        box-shadow: none;
      }
      .btn .pill {
        padding: 3px 8px;
        border-radius: 999px;
        border: 1px solid var(--line);
        color: var(--muted);
        font-weight: 600;
        font-size: 12px;
      }

      .layout {
        display: grid;
        grid-template-columns: 1.35fr 0.65fr;
        gap: 18px;
        margin-top: 16px;
      }
      @media (max-width: 980px) {
        .layout { grid-template-columns: 1fr; }
      }

      .card {
        background: rgba(255, 255, 255, 0.03);
        border: 1px solid var(--line);
        border-radius: 16px;
        box-shadow: var(--shadow);
        padding: 18px;
      }

      .card h2 {
        margin: 0 0 10px;
        font-size: 16px;
        letter-spacing: -0.01em;
      }

      .card h3 {
        margin: 18px 0 8px;
        font-size: 14px;
        letter-spacing: -0.01em;
        color: var(--text);
      }

      .card p {
        margin: 0;
        color: var(--muted);
        font-size: 14px;
      }

      .section { margin-top: 18px; }

      .figure {
        display: grid;
        gap: 10px;
        margin-top: 12px;
      }
      .figure img {
        width: 100%;
        height: auto;
        border-radius: 14px;
        border: 1px solid var(--line);
        background: rgba(255, 255, 255, 0.02);
      }
      .figcap {
        color: var(--muted);
        font-size: 13px;
      }

      .imgfail {
        border: 1px solid var(--line);
        border-radius: 14px;
        background: rgba(255, 255, 255, 0.02);
        padding: 12px;
        color: var(--muted);
        font-size: 13px;
      }

      .tablewrap {
        margin-top: 10px;
        overflow: auto;
        border: 1px solid var(--line);
        border-radius: 12px;
        background: rgba(255, 255, 255, 0.02);
      }
      table {
        width: 100%;
        border-collapse: collapse;
        font-size: 13px;
        min-width: 780px;
      }
      th, td {
        padding: 10px 10px;
        border-bottom: 1px solid var(--line);
        text-align: left;
        vertical-align: middle;
        white-space: nowrap;
      }
      th {
        color: var(--text);
        font-weight: 700;
        background: rgba(255, 255, 255, 0.03);
      }
      td { color: var(--muted); }
      tr:last-child td { border-bottom: none; }

      .strong { color: var(--text); font-weight: 700; }
      .u { text-decoration: underline; text-decoration-thickness: 1px; text-underline-offset: 3px; }

      .bibtex {
        margin-top: 10px;
        background: #0a0b0f;
        border: 1px solid var(--line);
        border-radius: 12px;
        padding: 12px;
        overflow: auto;
        font-size: 12px;
        color: #dfe7ff;
      }

      .note {
        margin-top: 10px;
        padding: 10px 12px;
        border: 1px solid var(--line);
        border-radius: 12px;
        background: rgba(255, 255, 255, 0.02);
        color: var(--muted);
        font-size: 13px;
      }

      footer {
        margin-top: 22px;
        color: var(--muted);
        font-size: 12px;
        display: flex;
        justify-content: space-between;
        flex-wrap: wrap;
        gap: 10px;
        border-top: 1px solid var(--line);
        padding-top: 16px;
      }

      .smalllink { color: var(--muted); }
      .smalllink:hover { color: var(--text); }

      .sr-only {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0,0,0,0);
        white-space: nowrap;
        border: 0;
      }

      code {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        font-size: 12px;
        color: #dfe7ff;
      }
    </style>

    <script>
      window.MathJax = {
        tex: { inlineMath: [["$", "$"], ["\\(", "\\)"]] },
        options: { skipHtmlTags: ["script","noscript","style","textarea","pre","code"] }
      };
    </script>
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>

  <body>
    <div class="wrap">
      <header>
        <div class="topbar">
          <div class="badge" aria-label="project status">
            <div class="dot" aria-hidden="true"></div>
            <span>ArXiv Preprint</span>
          </div>

          <nav class="nav" aria-label="page">
            <a href="#abstract">Abstract</a>
            <a href="#introduction">Introduction</a>
            <a href="#method">Method</a>
            <a href="#experiments">Experiments</a>
            <a href="#tables">Tables</a>
            <a href="#figures">Figures</a>
            <a href="#bibtex">BibTeX</a>
          </nav>
        </div>

        <h1>MVP-LAM</h1>
        <p class="subtitle">
          Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction
        </p>

        <div class="meta">
          <div class="authors">
            <a href="PASTE_PROFILE_LINK_HERE">Jung Min Lee</a> ·
            <a href="PASTE_PROFILE_LINK_HERE">Dohyeok Lee</a> ·
            <a href="PASTE_PROFILE_LINK_HERE">Seokhun Ju</a> ·
            <a href="PASTE_PROFILE_LINK_HERE">Taehyun Cho</a> ·
            <a href="PASTE_PROFILE_LINK_HERE">Jin Woo Koo</a> ·
            <a href="PASTE_PROFILE_LINK_HERE">Li Zhao</a> ·
            <a href="PASTE_PROFILE_LINK_HERE">Sangwoo Hong</a> ·
            <a href="PASTE_PROFILE_LINK_HERE">Jungwoo Lee</a>
          </div>
          <div class="affils">
            Seoul National University · Konkuk University · Microsoft Research Asia · HodooAI Labs
          </div>
        </div>

        <div class="cta" role="group" aria-label="links">
          <a class="btn primary disabled" href="#" aria-disabled="true" tabindex="-1">
            <span>arXiv</span>
            <span class="pill">Coming soon</span>
          </a>
          <a class="btn disabled" href="#" aria-disabled="true" tabindex="-1">
            <span>Code</span>
            <span class="pill">Coming soon</span>
          </a>
          <button class="btn" id="copy-bib" type="button">
            <span>Copy BibTeX</span>
            <span class="pill">1 click</span>
          </button>
        </div>
      </header>

      <div class="layout">
        <main class="card">
          <h2 id="abstract">Abstract</h2>
          <p>
            Learning <em>latent actions</em> from diverse human videos enables scaling robot learning beyond embodiment-specific robot datasets, and these latent actions have recently been used as pseudo-action labels for vision-language-action (VLA) model pretraining.
            To make VLA pretraining effective, latent actions should contain information about the underlying agent's actions despite the absence of ground-truth labels.
            We propose <strong>M</strong>ulti-<strong>V</strong>iew<strong>P</strong>oint <strong>L</strong>atent <strong>A</strong>ction <strong>M</strong>odel (<strong>MVP-LAM</strong>), which learns discrete latent actions that are highly informative about ground-truth actions from time-synchronized multi-view videos.
            MVP-LAM trains latent actions with a <em>cross-viewpoint reconstruction</em> objective, so that a latent action inferred from one view must explain the future in another view, reducing reliance on viewpoint-specific cues.
            On Bridge V2, MVP-LAM produces more action-centric latent actions, achieving higher mutual information with ground-truth actions and improved action prediction, including under out-of-distribution evaluation.
            Finally, pretraining VLAs with MVP-LAM latent actions improves downstream manipulation performance on the SIMPLER and LIBERO-Long benchmarks.
          </p>

          <div class="section">
            <h2 id="introduction">Introduction</h2>
            <p>
              Collecting real-world robot demonstrations remains a central bottleneck in training generalist manipulation policies.
              Unlike foundation models in other domains, robot learning is constrained by the cost of acquiring action-labeled trajectories, which typically requires human teleoperation.
              This makes large-scale data collection slow and expensive, and the resulting datasets often depend on a specific embodiment and sensor setup.
              To alleviate this limitation, learning from video has emerged as a promising alternative that exploits abundant human manipulation videos to acquire transferable priors over manipulation-relevant dynamics.
              A fundamental challenge, however, is that such videos do not provide low-level robot action labels, preventing standard supervised imitation learning.
            </p>
            <div class="section">
              <p>
                A key obstacle for action-centric latent actions is exogenous noise where visual transitions can be spuriously influenced by factors other than the agent's actions yet still correlate with frame-to-frame changes.
                Among these factors, we focus on viewpoint variation.
                Viewpoint changes introduce camera movements and perspective shifts, entangling visual transitions with the agent's action.
                As a result, latent actions learned from single-view reconstruction can overfit to viewpoint-dependent cues and become less predictive of the actions.
              </p>
            </div>
            <div class="section">
              <p>
                We propose Multi-ViewPoint Latent Action Model, which learns discrete latent actions that are highly informative about ground-truth actions.
                MVP-LAM is trained on time-synchronized multi-view videos with a cross-viewpoint reconstruction objective, where a latent action inferred from one view is used to predict the future observation in another view.
                This discourages latent actions from encoding the viewpoint-specific information and achieves more action-centric latent actions.
              </p>
            </div>
          </div>

          <div class="section">
            <h2 id="method">Method</h2>

            <h3>Action-centric Latent Action</h3>
            <p>
              When latent actions are used as pseudo-action labels for behavior cloning policies, it is desirable that the learned latent action $Z_t$ preserves as much information as possible about the underlying action $A_t$.
              We denote the state by $S_t$, and assume an expert policy induces actions $A_t \sim \pi^\star(\cdot \mid S_t)$ for a given task.
              In the pretraining stage, we typically do not observe $S_t$ or $A_t$.
              Instead, we only observe images or their features $O_t = f(I_t)$.
              LAM produces latent actions from consecutive observations, $Z_t = E_\\theta(O_t, O_{t+1})$ with vector quantization when using VQ-VAE.
            </p>

            <div class="section">
              <p>
                We define a latent action $Z_t$ action-centric if it is highly informative about the underlying action $A_t$.
                We quantify this by mutual information and consider the objective
                $$\\max_{Z_t} \\, \\mathcal{I}(Z_t; A_t).$$
                In this context, viewpoint variation acts as noise.
                Changes in camera pose $V_t$ can induce frame-to-frame differences in $O_t$ that are predictive of $Z_t$ but are not caused by the action $A_t$.
                When $Z_t$ is learned under a limited-capacity bottleneck such as vector quantization, allocating representational capacity to viewpoint-dependent factors can come at the expense of action-relevant dynamics and reduce $\\mathcal{I}(Z_t;A_t)$.
              </p>
            </div>

            <h3>Multi-Viewpoint Latent Action Learning</h3>
            <p>
              MVP-LAM leverages time-synchronized multi-view videos and cross-viewpoint reconstruction to learn action-centric latent actions.
              For clarity, we describe the two-view case but note that the objective extends to more views.
            </p>
          </div>

          <div class="section">
            <h2 id="experiments">Experiments</h2>

            <h3><strong>RQ1.</strong> Are MVP-LAM latent actions more action-centric</h3>
            <p>
              We evaluate how action-centric a latent action is by measuring mutual information between latent actions and ground-truth actions, and how well actions can be predicted from latent actions with a simple linear layer.
            </p>

            <div class="section">
              <p>
                As shown in the results below, MVP-LAM achieves the highest estimated $\\mathcal{I}(Z;A)$ across estimators.
                The linear probe also yields lower NMSE on Bridge V2 and on out-of-distribution LIBERO suites Spatial Object Long, with a small drop on LIBERO-Goal relative to UniVLA.
              </p>
            </div>

            <h3><strong>RQ2.</strong> Is MVP-LAM Effective for Manipulation</h3>
            <div class="section">
              <p>
                Pretraining with MVP-LAM latent actions improves downstream manipulation.
                The average success rate increases from 39.6 percent to 60.4 percent on SIMPLER.
                On LIBERO-Long, MVP-LAM achieves 90.8 percent success, improving over UniVLA pretrained on Bridge V2 at 79.4 percent.
              </p>
            </div>

            <h3><strong>RQ3.</strong> Does MVP-LAM Preserve Transition Information Under Viewpoint Perturbation</h3>
            <div class="section">
              <p>
                $$\\mathrm{MSE}=\\left\\lVert o_{t+1}^{\\text{dino}}-\\hat{o}_{t+1}^{\\text{dino}}(z_t) \\right\\rVert_2^2, \\quad
                \\widetilde{\\mathrm{MSE}}=\\left\\lVert o_{t+1}^{\\text{dino}}-\\hat{o}_{t+1}^{\\text{dino}}(\\tilde{z}_t) \\right\\rVert_2^2.$$
                Concretely, MSE uses $z_t$ from the original transition, whereas $\\widetilde{\\mathrm{MSE}}$ uses $\\tilde{z}_t$ from the viewpoint-perturbed transition.
                A larger $\\widetilde{\\mathrm{MSE}}$ suggests that the latent action depends on viewpoint variation.
              </p>
            </div>
          </div>

          <div class="section">
            <h2 id="tables">Tables</h2>

            <h3>SIMPLER benchmark result</h3>
            <p class="note">
              We report success rate and grasping rate (%) on the SIMPLER benchmark.
              † denotes results reported in prior work.
              Best is bolded and second best is underlined.
            </p>

            <div class="tablewrap">
              <table aria-label="SIMPLER benchmark result table">
                <thead>
                  <tr>
                    <th>Success Rate</th>
                    <th>MVP-LAM</th>
                    <th>UniVLA</th>
                    <th>LAPA†</th>
                    <th>OpenVLA†</th>
                    <th>Octo-Small</th>
                    <th>Octo-Base</th>
                    <th>$\\pi_{0}$</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="strong">StackG2Y</td>
                    <td>33.3</td>
                    <td>16.7</td>
                    <td class="strong">54.2</td>
                    <td class="u">41.6</td>
                    <td>8.3</td>
                    <td>0.0</td>
                    <td>37.5</td>
                  </tr>
                  <tr>
                    <td class="strong">Carrot2Plate</td>
                    <td class="strong">66.7</td>
                    <td>20.8</td>
                    <td>45.8</td>
                    <td class="u">50.0</td>
                    <td>33.3</td>
                    <td>37.5</td>
                    <td>33.3</td>
                  </tr>
                  <tr>
                    <td class="strong">Spoon2Towel</td>
                    <td class="u">66.7</td>
                    <td>54.2</td>
                    <td class="strong">70.8</td>
                    <td>37.5</td>
                    <td>25.0</td>
                    <td>12.5</td>
                    <td>29.2</td>
                  </tr>
                  <tr>
                    <td class="strong">Eggplant2Bask</td>
                    <td class="strong">75.0</td>
                    <td class="u">66.7</td>
                    <td>58.3</td>
                    <td>16.7</td>
                    <td>12.5</td>
                    <td>20.8</td>
                    <td>45.8</td>
                  </tr>
                  <tr>
                    <td class="strong">AVG</td>
                    <td class="strong">60.4</td>
                    <td>39.6</td>
                    <td class="u">57.3</td>
                    <td>36.4</td>
                    <td>19.8</td>
                    <td>17.7</td>
                    <td>36.5</td>
                  </tr>

                  <tr>
                    <th>Grasping Rate</th>
                    <th colspan="7"></th>
                  </tr>

                  <tr>
                    <td class="strong">StackG2Y</td>
                    <td>54.3</td>
                    <td>45.8</td>
                    <td class="u">62.5</td>
                    <td>50.0</td>
                    <td>54.2</td>
                    <td class="strong">70.8</td>
                    <td>58.3</td>
                  </tr>
                  <tr>
                    <td class="strong">Carrot2Plate</td>
                    <td class="u">70.8</td>
                    <td>37.5</td>
                    <td>58.3</td>
                    <td>66.6</td>
                    <td class="strong">75.0</td>
                    <td>54.2</td>
                    <td>58.3</td>
                  </tr>
                  <tr>
                    <td class="strong">Spoon2Towel</td>
                    <td class="u">79.2</td>
                    <td class="u">79.2</td>
                    <td class="strong">83.3</td>
                    <td>45.8</td>
                    <td>66.7</td>
                    <td>70.8</td>
                    <td>54.2</td>
                  </tr>
                  <tr>
                    <td class="strong">Eggplant2Bask</td>
                    <td class="u">95.8</td>
                    <td class="strong">100.0</td>
                    <td>83.3</td>
                    <td>37.5</td>
                    <td>50.0</td>
                    <td>54.2</td>
                    <td>87.5</td>
                  </tr>
                  <tr>
                    <td class="strong">AVG</td>
                    <td class="strong">75.0</td>
                    <td>65.6</td>
                    <td class="u">71.9</td>
                    <td>50.0</td>
                    <td>61.5</td>
                    <td>62.5</td>
                    <td>64.6</td>
                  </tr>
                </tbody>
              </table>
            </div>

            <h3>LIBERO-Long results</h3>
            <p class="note">
              Success rate (%) on LIBERO-Long.
              † indicates results reported in prior work and * indicates methods that use additional wrist-view images and states.
              Best is bolded and second best is underlined.
            </p>

            <div class="tablewrap">
              <table aria-label="LIBERO-Long results table">
                <thead>
                  <tr>
                    <th>MVP-LAM</th>
                    <th>UniVLA (Bridge)</th>
                    <th>OpenVLA †</th>
                    <th>$\\pi_0$ † *</th>
                    <th>UniVLA † (OXE)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="strong">90.8</td>
                    <td class="u">79.4</td>
                    <td>53.7</td>
                    <td>85.2</td>
                    <td>92.0</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <div class="section">
            <h2 id="bibtex">BibTeX</h2>
            <pre class="bibtex" id="bibtex-block" aria-label="bibtex">
@misc{lee2026mvplam,
  title  = {MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction},
  author = {Lee, Jung Min and Lee, Dohyeok and Ju, Seokhun and Cho, Taehyun and Koo, Jin Woo and Zhao, Li and Hong, Sangwoo and Lee, Jungwoo},
  year   = {2026},
  note   = {arXiv preprint, coming soon}
}
            </pre>
            <span class="sr-only" id="copy-status" aria-live="polite"></span>
          </div>
        </main>

        <aside class="card" id="figures">
          <h2>Figures</h2>

          <div class="figure">
            <img data-asset="concept.png" alt="Concept figure" />
            <div class="figcap">
              <strong>Why viewpoint variation interferes with learning latent actions.</strong>
              Viewpoint variation acts as noise.
              Frame-to-frame visual differences reflect both interaction-driven state changes and viewpoint-dependent appearance changes (e.g., camera movements).
              Because these factors are entangled, the same underlying action can induce different visual transitions across viewpoints.
              This confounding makes it difficult to learn latent actions that are consistently predictive of the underlying control actions.
            </div>
          </div>

          <div class="figure">
            <img data-asset="arch.png" alt="Architecture figure" />
            <div class="figcap">
              <strong>MVP-LAM training with time-synchronized multi-view videos.</strong>
              (1) <em>Self-viewpoint reconstruction</em> (left): for each view $v$, frozen DINOv2 extracts features $(o_t^v,o_{t+1}^v)$.
              A spatiotemporal encoder produces a continuous latent $e_t^v$ that is vector-quantized into a discrete token $z_t^v$, and a decoder reconstructs $o_{t+1}^v$ from $(o_t^v,z_t^v)$.
              (2) <em>Cross-viewpoint reconstruction</em> (right): MVP-LAM swaps latent tokens across views (e.g., $z_t^{v_1}\\leftrightarrow z_t^{v_2}$) while reconstructing each view’s future feature, encouraging $z_t$ to capture inherent transition information.
            </div>
          </div>

          <div class="figure">
            <img data-asset="mi.png" alt="Mutual information figure" />
            <div class="figcap">
              <strong>Estimated mutual information.</strong>
              $\\mathcal{I}(Z;A)$ on Bridge V2 with KSG, BA, and MINE estimators.
              For KSG, latent actions are randomly projected to $d{=}256$ prior to estimation.
              Higher is better.
              Error bars show standard deviation over four seeds.
            </div>
          </div>

          <div class="figure">
            <img data-asset="nmse_bridge_libero_clean_all_oneplot.png" alt="Linear probing figure" />
            <div class="figcap">
              <strong>Linear probing result.</strong>
              NMSE of a linear layer predicting actions from latent actions.
              Bridge V2 is in-distribution; LIBERO (Spatial/Object/Goal/Long) is out-of-distribution.
              Lower is better.
              Error bars show standard deviation over four seeds.
            </div>
          </div>

          <div class="figure">
            <img data-asset="nvs_res_traj.png" alt="Viewpoint perturbed trajectory figure" />
            <div class="figcap">
              <strong>Viewpoint perturbed trajectory and evaluation.</strong>
              (Left): an example trajectory from the original camera view $\\{I_t\\}$ (top) and its viewpoint perturbed counterpart $\\{\\tilde{I}_t\\}$ (bottom).
              (Right): reconstruction error on the original and perturbed sequences (top), reporting $\\mathrm{MSE}$ and $\\widetilde{\\mathrm{MSE}}$ and action-centricity metrics (bottom), reporting KSG mutual information and NMSE of linear probing, for MVP-LAM and baselines.
              Error bars show standard deviation over 3 random seeds.
            </div>
          </div>

          <div class="note">
            Put images in a folder named <code>assets</code> or <code>asset</code>.
            Filenames must match exactly.
          </div>
        </aside>
      </div>

      <footer>
        <div>© <span id="year"></span> MVP-LAM</div>
        <div>
          <a class="smalllink" href="https://latentactionpretraining.github.io/">latentactionpretraining.github.io</a>
        </div>
      </footer>
    </div>

    <script>
      (function () {
        var yearEl = document.getElementById("year");
        if (yearEl) yearEl.textContent = String(new Date().getFullYear());

        var copyBtn = document.getElementById("copy-bib");
        var bib = document.getElementById("bibtex-block");
        var status = document.getElementById("copy-status");

        function setStatus(msg) {
          if (!status) return;
          status.textContent = msg;
          setTimeout(function () { status.textContent = ""; }, 1200);
        }

        if (copyBtn && bib) {
          copyBtn.addEventListener("click", async function () {
            var text = bib.textContent.trim();
            try {
              await navigator.clipboard.writeText(text);
              setStatus("BibTeX copied");
            } catch (e) {
              var ta = document.createElement("textarea");
              ta.value = text;
              document.body.appendChild(ta);
              ta.select();
              document.execCommand("copy");
              document.body.removeChild(ta);
              setStatus("BibTeX copied");
            }
          });
        }

        function tryLoad(img, paths, idx) {
          if (idx >= paths.length) {
            var box = document.createElement("div");
            box.className = "imgfail";
            box.textContent = "Image missing. Expected one of these paths: " + paths.join(" , ");
            img.replaceWith(box);
            return;
          }
          img.src = paths[idx];
          img.onerror = function () { tryLoad(img, paths, idx + 1); };
        }

        var imgs = document.querySelectorAll("img[data-asset]");
        imgs.forEach(function (img) {
          var name = img.getAttribute("data-asset");
          var paths = [
            "assets/" + name,
            "asset/" + name,
            "./assets/" + name,
            "./asset/" + name
          ];
          tryLoad(img, paths, 0);
        });
      })();
    </script>
  </body>
</html>
